{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgHK6kq9s639"
      },
      "source": [
        "# Introduction\n",
        "In the following notebook, I will be using the IMDB sentiment analysis dataset from keras to create a neural network, which is able to accurately classify positive vs. negative reviews. The dataset includes around 50,000 reviews from IMDB, which are labeled as positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKcO0wUlt792"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0S2SqXVDswLu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # necessary for importing keras\n",
        "from tensorflow import keras # keras includes the dataset and other neural network tools we're using\n",
        "from keras.datasets import imdb # imdb dataset\n",
        "import numpy as np # for defining arrays\n",
        "from keras.models import Sequential # for modeling\n",
        "from keras.wrappers.scikit_learn import KerasClassifier # for creating the model\n",
        "from keras import layers # for defining layers in model\n",
        "from keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense # for model parameters\n",
        "from sklearn.model_selection import ParameterGrid # for creating the model parameter grid\n",
        "from keras.callbacks import EarlyStopping # used in parameter grid\n",
        "from sklearn.metrics import accuracy_score # for calculating test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc96n7hIuFHA"
      },
      "source": [
        "# Splitting data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CzT-72_-uAfZ"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe87we_su3ag"
      },
      "source": [
        "# Decoding the reviews\n",
        "This dataset is a bit confusing since the text of the reviews are actually integer-encoded. This means that the value of each review is just a sequence of integers with each integer representing a specific word in the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alP9zerMvcab"
      },
      "source": [
        "## Example review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJOmLlAdvgnL",
        "outputId": "6530e1ae-7231-4960-dd38-0352adaba020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg--KXzgvwP_"
      },
      "source": [
        "## Decoding from integer to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "P6A5zPRku0mQ",
        "outputId": "2a91386c-f06e-4001-d31e-53897c2eb8e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_index = imdb.get_word_index()\n",
        "index_from = 3\n",
        "word_index = {key:(value+index_from) for key,value in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "\n",
        "reverse_word_index = {value:key for key, value in word_index.items()}\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# source: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBRerVXqxscf"
      },
      "source": [
        "## Same review as above, but now decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "vcntfqejxwhb",
        "outputId": "51981905-aa06-469d-9e40-2e008a26fdf3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode_review(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnvh1P_uyvhL"
      },
      "source": [
        "# Defining train, validation, and test sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plZ-H8pIx6DA",
        "outputId": "55a70188-2f8e-4408-bafa-e0a7c166c5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22500\n",
            "2500\n",
            "25000\n"
          ]
        }
      ],
      "source": [
        "total_words = 2500 # Only using the 2500 most common words (you can use any amount, I just decided to use 2500)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words= total_words)\n",
        "\n",
        "X_train, X_val = X_train[:-2500], X_train[-2500:]\n",
        "y_train, y_val = y_train[:-2500], y_train[-2500:]\n",
        "\n",
        "print(len(X_train)) # number of training sequences\n",
        "print(len(X_val)) # number of validation sequences\n",
        "print(len(X_test)) # number of test sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLXh8STWz6aI"
      },
      "source": [
        "## Same review as before, but now with less common words removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ERlDYhFVzR6q",
        "outputId": "5cf4775c-30ac-40ab-93ec-7a868a6dd01d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> island as myself so i loved the fact there was a real connection with this film the witty <UNK> throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really <UNK> at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of <UNK> and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was <UNK> life after all that was <UNK> with us all\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode_review(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5wniwcp1QTk"
      },
      "source": [
        "# Standardize length of reviews:\n",
        "Here, we will be using the pad_sequences function from keras to standardize the length of the reviews in this dataset. We do this because the length of the reviews can vary greatly and this can effect the accuracy of our sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNw-yjP30tMd",
        "outputId": "34d3b076-e284-4aad-e05e-8df9358f2ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(22500, 500)\n",
            "(2500, 500)\n",
            "(25000, 500)\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 500 # setting a 500 word limit for all reviews\n",
        "\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, value= word_index[\"<PAD>\"], padding= 'post', maxlen= max_sequence_length)\n",
        "X_val = keras.preprocessing.sequence.pad_sequences(X_val, value= word_index[\"<PAD>\"], padding= 'post', maxlen= max_sequence_length)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, value= word_index[\"<PAD>\"], padding= 'post', maxlen= max_sequence_length)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "# source: https://note.com/mlai/n/ndd0643e2a843"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM6LbtaJ5PxU"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwmKHhKh5PLr",
        "outputId": "1ff25d5f-9e25-4720-ec60-c74e9df8e957"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-7665640bcc8f>:25: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn= create_model)\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "def create_model(filters = 64, kernel_size = 3, strides=1, units = 256,\n",
        "                 optimizer='adam', rate = 0.25, kernel_initializer ='glorot_uniform'): # model parameters\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(total_words, embedding_dim, input_length= max_sequence_length)) # embedding layer\n",
        "\n",
        "    model.add(Dropout(rate))\n",
        "    model.add(Conv1D(filters = filters, kernel_size = kernel_size, strides= strides,\n",
        "                     padding='same', activation= 'relu')) # convolutional layers\n",
        "\n",
        "    model.add(GlobalMaxPooling1D()) # max pooling\n",
        "\n",
        "    model.add(Dense(units = units, activation= 'relu', kernel_initializer= kernel_initializer)) # dense layer\n",
        "    model.add(Dropout(rate)) # dropout\n",
        "\n",
        "    model.add(Dense(1, activation= 'sigmoid')) # output layer\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer= optimizer,\n",
        "                  metrics=['accuracy']) # compile the model\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn= create_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SDJ4Msn6hq9"
      },
      "source": [
        "## Tuning hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge-q7KGA6J9k",
        "outputId": "e0d9d879-4466-4c1b-a787-809ce119d828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_initializer': 'TruncatedNormal', 'kernel_size': 5, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 128}\n",
            "Epoch 1/5\n",
            "352/352 [==============================] - 42s 115ms/step - loss: 0.5445 - accuracy: 0.6900 - val_loss: 0.3451 - val_accuracy: 0.8576\n",
            "Epoch 2/5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3170 - accuracy: 0.8672 - val_loss: 0.3025 - val_accuracy: 0.8728\n",
            "Epoch 3/5\n",
            "352/352 [==============================] - 36s 103ms/step - loss: 0.2671 - accuracy: 0.8910 - val_loss: 0.2856 - val_accuracy: 0.8824\n",
            "Epoch 4/5\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2333 - accuracy: 0.9070 - val_loss: 0.2869 - val_accuracy: 0.8836\n",
            "Epoch 5/5\n",
            "352/352 [==============================] - 37s 107ms/step - loss: 0.2107 - accuracy: 0.9186 - val_loss: 0.2803 - val_accuracy: 0.8832\n",
            "Epoch 5: early stopping\n",
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "{'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_initializer': 'TruncatedNormal', 'kernel_size': 5, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 512}\n",
            "Epoch 1/5\n",
            "352/352 [==============================] - 38s 106ms/step - loss: 0.5158 - accuracy: 0.7132 - val_loss: 0.3524 - val_accuracy: 0.8528\n",
            "Epoch 2/5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3170 - accuracy: 0.8642 - val_loss: 0.3040 - val_accuracy: 0.8768\n",
            "Epoch 3/5\n",
            "352/352 [==============================] - 39s 110ms/step - loss: 0.2617 - accuracy: 0.8911 - val_loss: 0.2722 - val_accuracy: 0.8936\n",
            "Epoch 4/5\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2297 - accuracy: 0.9059 - val_loss: 0.3049 - val_accuracy: 0.8796\n",
            "Epoch 4: early stopping\n",
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "param_scores: [0.8831999897956848, 0.8795999884605408]\n",
            "best parameter set {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_initializer': 'TruncatedNormal', 'kernel_size': 5, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 128}\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "filters = [128]\n",
        "kernel_size = [5]\n",
        "strides = [1]\n",
        "Dense_units = [128, 512]\n",
        "kernel_initializer = ['TruncatedNormal']\n",
        "rate_dropouts = [0.25]\n",
        "optimizers = ['adam']\n",
        "epochs = [5]\n",
        "batches = [64]\n",
        "\n",
        "# parameter grid search\n",
        "param_grid = dict(optimizer = optimizers, epochs = epochs, batch_size = batches,\n",
        "                  filters = filters, kernel_size = kernel_size, strides = strides,\n",
        "                  units = Dense_units, kernel_initializer = kernel_initializer, rate = rate_dropouts)\n",
        "\n",
        "grid = ParameterGrid(param_grid)\n",
        "param_sets = list(grid)\n",
        "\n",
        "param_scores = []\n",
        "for params in grid:\n",
        "\n",
        "    print(params)\n",
        "    model.set_params(**params)\n",
        "\n",
        "    earlystopper = EarlyStopping(monitor='val_accuracy', patience= 0, verbose=1)\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        shuffle= True,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks= [earlystopper])\n",
        "\n",
        "    param_score = history.history['val_accuracy']\n",
        "    param_scores.append(param_score[-1])\n",
        "    print('+-'*50)\n",
        "\n",
        "print('param_scores:', param_scores)\n",
        "\n",
        "# Choose best parameters\n",
        "p = np.argmax(np.array(param_scores))\n",
        "best_params = param_sets[p]\n",
        "print(\"best parameter set\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFX6FV-hZrPz"
      },
      "source": [
        "## Running model with best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sRq7FoL_fjs",
        "outputId": "8630897b-d7df-4806-b004-978055b05e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 40s 99ms/step - loss: 0.5376 - accuracy: 0.7028\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 40s 103ms/step - loss: 0.3114 - accuracy: 0.8688\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2583 - accuracy: 0.8967\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2313 - accuracy: 0.9099\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 41s 104ms/step - loss: 0.2084 - accuracy: 0.9195\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7abf7d9bba00>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.set_params(**best_params)\n",
        "model.fit(np.vstack((X_train, X_val)), np.hstack((y_train, y_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnWmrYdaZuZi"
      },
      "source": [
        "## Test accuracy of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3F4qET8_h29",
        "outputId": "886519f2-a2ea-4c3d-83e3-c3e44f4cda03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 17s 21ms/step\n",
            "Test accuracy = 81.876000%\n"
          ]
        }
      ],
      "source": [
        "print(\"Test accuracy = %f%%\" % (accuracy_score(y_test, model.predict(X_test))*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEVmxQfAadV9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
